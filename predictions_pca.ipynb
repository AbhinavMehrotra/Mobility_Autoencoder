{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.0_4\n",
      "0.2.0_4\n"
     ]
    }
   ],
   "source": [
    "run FunctionsLoader.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Predictions():\n",
    "    \n",
    "    \n",
    "    def __init__(self, min_days=30):\n",
    "        self.min_days = min_days\n",
    "            \n",
    "        \n",
    "    # time_window should be in mins (e.g; 30mins)\n",
    "    # sd_ratio should be between 0 and 1 (e.g; 0.25 for using 1/4 of sd)\n",
    "\n",
    "    def loadData(self, time_window, sd_ratio, t_hist):\n",
    "        \n",
    "        ### read mobility data ###\n",
    "        ld = LocationData()\n",
    "        ld.readLocationData(precision_value=4) # read file\n",
    "        ld.filterUser(self.min_days) #filter users with min 30 days\n",
    "        #print len(ld.users)\n",
    "        ld.convertToEqualTimeSeries(time_window * 60 * 1000) # convert to time series of 30mins\n",
    "        dt = ld.createUserDailyTrajectoryLists() # create daily trajectories of users \n",
    "\n",
    "        ### read mood data ###\n",
    "        pd = PhqData()\n",
    "        pd.readPhqData() \n",
    "        pd.computeScores(ld.users, self.min_days)\n",
    "        pd.computeDivergence()\n",
    "        pd.convertTo2Labels(sd_ratio=sd_ratio) \n",
    "        #len(list(set(m[0] for m in pd.phq_scores)))\n",
    "\n",
    "        ### merge data ###\n",
    "        mlm = PhqLocMerge()\n",
    "        mlm.mergeData(dt, pd.phq_scores, t_hist)\n",
    "        \n",
    "        ### set data as a class variable ###\n",
    "        self.data = mlm.data\n",
    "\n",
    "    \n",
    "    \n",
    "    def prepareInput(self):\n",
    "    \n",
    "        ###  transform mobility data ### \n",
    "        users = list(set([m[0] for m in self.data]))\n",
    "        dt2 = approachAvg2(lm_data=self.data, users=users) # Computing displacement changes\n",
    "        dt3 = approachAvg3(lm_data=self.data, users=users) # Computing displacement [normalised]\n",
    "        dt4 = approachAvg4(lm_data=self.data, users=users, topn=20) # Computing displacement [normalised]\n",
    "\n",
    "        ###  combinations of data for using different combinations of features ### \n",
    "        self.data1 = [dt2] \n",
    "        self.data2 = [dt3] \n",
    "        self.data3 = [dt4] \n",
    "        self.data4 = [dt2,dt3] \n",
    "        self.data5 = [dt2,dt4] \n",
    "        self.data6 = [dt3,dt4] \n",
    "        self.data7 = [dt2,dt3,dt4]\n",
    "    \n",
    "    \n",
    "    def computePredictions(self, encoded_layer_size=10, classifier_type=1):\n",
    "\n",
    "        ### filter users ### \n",
    "        filtered_users = []\n",
    "        users = list(set([m[0] for m in self.data]))\n",
    "        for u in users:\n",
    "            ud = [d for d in self.data1[0] if d[0]==u]\n",
    "            if len(ud) > self.min_days:\n",
    "                filtered_users.append(u)\n",
    "        print 'Number of users', len(filtered_users)\n",
    "        \n",
    "        ### computing prdiction accuracy ### \n",
    "        model1 = PCAClassifierPersonalisedHelper.compute_svm_accuracy(self.data1, iteration=20, classifier_type=classifier_type, users=filtered_users, min_days=self.min_days, encoded_layer_size=encoded_layer_size)\n",
    "        model2 = PCAClassifierPersonalisedHelper.compute_svm_accuracy(self.data2, iteration=20, classifier_type=classifier_type, users=filtered_users, min_days=self.min_days, encoded_layer_size=encoded_layer_size) \n",
    "        model3 = PCAClassifierPersonalisedHelper.compute_svm_accuracy(self.data3, iteration=20, classifier_type=classifier_type, users=filtered_users, min_days=self.min_days, encoded_layer_size=encoded_layer_size) \n",
    "        model4 = PCAClassifierPersonalisedHelper.compute_svm_accuracy(self.data4, iteration=20, classifier_type=classifier_type, users=filtered_users, min_days=self.min_days, encoded_layer_size=encoded_layer_size) \n",
    "        model5 = PCAClassifierPersonalisedHelper.compute_svm_accuracy(self.data5, iteration=20, classifier_type=classifier_type, users=filtered_users, min_days=self.min_days, encoded_layer_size=encoded_layer_size) \n",
    "        model6 = PCAClassifierPersonalisedHelper.compute_svm_accuracy(self.data6, iteration=20, classifier_type=classifier_type, users=filtered_users, min_days=self.min_days, encoded_layer_size=encoded_layer_size) \n",
    "        model7 = PCAClassifierPersonalisedHelper.compute_svm_accuracy(self.data7, iteration=20, classifier_type=classifier_type, users=filtered_users, min_days=self.min_days, encoded_layer_size=encoded_layer_size) \n",
    "        \n",
    "        self.models = [model1, model2, model3, model4, model5, model6, model7]\n",
    "\n",
    "        \n",
    "    def plotPredictions(self, include_mt=False, t_hist=14, file_path=''):\n",
    "#         model_names = ['IR 1','IR 2','IR 3','IR 1+2','IR 1+3','IR 2+3','IR 1+2+3']\n",
    "        model_names = ['IR 1','IR 2','IR 3','IR 1+2','IR 1+3','IR 2+3','IR 1+2+3']\n",
    "        label_dict = {1:'Sensitivity', 0:'Specificity'}\n",
    "        Plots.multiModel(models=p.models, model_names=model_names, label_dict=label_dict, include_mt=include_mt, t_hist=t_hist, file_path=file_path)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PCAClassifierPersonalised():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.frequency = {}\n",
    "        self.accuracy = -1\n",
    "        self.prf = []\n",
    "        self.conf_matrix = []\n",
    "        \n",
    "        \n",
    "    def classify(self, x_train, y_train, x_test, y_test, classifier_type): \n",
    "        \n",
    "        if len(x_train) != len(y_train) or len(x_test) != len(y_test):\n",
    "            raise ValueError(\"Length mismatch of data and labels\") \n",
    "        \n",
    "        ### compute frequency of labels in train data ###\n",
    "        self.frequency = dict([(i,0) for i in np.unique(y_train)])\n",
    "        for number in y_train:\n",
    "            self.frequency[number]+=1 \n",
    "        for key in self.frequency:\n",
    "            self.frequency[key] = float(self.frequency[key]) / len(y_train)\n",
    "\n",
    "            \n",
    "        #print \"Number of samples:\", len(x_train)\n",
    "        #print \"Number of features:\", len(x_train[0])\n",
    "        \n",
    "        ### setting up classifier ###\n",
    "        if classifier_type == 1:\n",
    "            clf = RandomForestClassifier(max_depth=10, random_state=0)\n",
    "        elif classifier_type == 2:\n",
    "            clf = svm.SVC(probability=False,  kernel=\"rbf\")\n",
    "        elif classifier_type == 3:\n",
    "            clf = GradientBoostingClassifier()\n",
    "#             clf = AdaBoostClassifier()\n",
    "            \n",
    "        clf.fit(x_train, y_train) \n",
    "\n",
    "        ### compute model performance for test data###\n",
    "        y_true = y_test\n",
    "        y_pred = clf.predict(x_test)\n",
    "        self.accuracy = sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\n",
    "        self.prf = sklearn.metrics.precision_recall_fscore_support(y_true, y_pred, beta=1.0, pos_label=1, average='weighted', sample_weight=None)\n",
    "        self.conf_matrix = sklearn.metrics.confusion_matrix(y_true, y_pred)\n",
    "        #print \"Performance computed: \", self.conf_matrix\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "    # x_data should have two columns for user and data\n",
    "    def classify_personalised(self, x_data_list, y_data, iteration, classifier_type, users, min_days, encoded_layer_size):\n",
    "        \n",
    "        \n",
    "        # train autoencoder & encode data\n",
    "        encoded_x_data_list = []\n",
    "        for xd in x_data_list:\n",
    "            x_data = []\n",
    "            for x in xd:\n",
    "                x_data.append(x[1])\n",
    "                \n",
    "            input_layer_size = len(x_data[0])\n",
    "            \n",
    "            ae = FeatureExtractionPCA(input_layer_size)\n",
    "            encoded_data = ae.encode_data(x_data)\n",
    "            x_data = encoded_data.tolist()\n",
    "            \n",
    "            if len(xd) != len(x_data):\n",
    "                print \"Check if both are equal\", len(xd), len(x_data)\n",
    "                stop\n",
    "            \n",
    "            temp_data = []\n",
    "            for i in range(len(x_data)):\n",
    "                item = [xd[i][0], x_data[i]]\n",
    "                temp_data.append(item)\n",
    "            encoded_x_data_list.append(temp_data)\n",
    "        x_data_list = encoded_x_data_list \n",
    "        #print \"Ecoded X data\"\n",
    "\n",
    "        \n",
    "        # get list of users\n",
    "        self.users = users\n",
    "        \n",
    "        k_frequency = []\n",
    "        k_accuracy = []\n",
    "        k_prf = []\n",
    "        k_conf_matrix = []\n",
    "        \n",
    "        \n",
    "        counter = 0\n",
    "        for i in range(len(self.users)):\n",
    "            current_user = self.users[i]\n",
    "            #print \"User: \", i, current_user\n",
    "        \n",
    "            # get user's y-data\n",
    "            u_y_data = []\n",
    "            for j in range(len(x_data_list[0])):\n",
    "                if x_data_list[0][j][0] == current_user:\n",
    "                    u_y_data.append(y_data[j])\n",
    "                    \n",
    "            if len(u_y_data) < min_days:\n",
    "                #print \"Not enought data for the user: \", len(u_y_data)\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            # get user's x-data\n",
    "            u_x_data = []\n",
    "            for xd in x_data_list:\n",
    "                if len(u_x_data) == 0:\n",
    "                    u_x_data = [d[1] for d in xd if d[0] == current_user]\n",
    "                    #print \"U-Data len\", len(u_x_data)\n",
    "                else:\n",
    "                    temp_data = [d[1] for d in xd if d[0] == current_user]\n",
    "                    for j in range(len(temp_data)):\n",
    "                        for d in temp_data[j]:\n",
    "                            u_x_data[j].append(d)\n",
    "                    #print \"U-Data len\", len(u_x_data)\n",
    "            \n",
    "            if len(u_x_data) != len(u_y_data):\n",
    "                print \"Unequal x & y lengths\", len(u_x_data), len(u_y_data)\n",
    "                stop\n",
    "            #print \"number of samples for the user:\", len(u_y_data)\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            for itr in range(iteration):\n",
    "                # setting up train & test data \n",
    "                all_train_data, all_test_data, all_train_labels, all_test_labels = cross_validation.train_test_split(\n",
    "                    u_x_data, u_y_data, test_size=0.2, random_state=itr)\n",
    "\n",
    "\n",
    "                # return is there not enough unique labels\n",
    "                if len(set(all_train_labels)) != len(set(all_test_labels)) or len(set(all_train_labels)) == 1:\n",
    "                    #print 'Not enough samples for this iteration'\n",
    "                    #print 'Train-test labels', len(set(all_train_labels)), len(set(all_test_labels))\n",
    "                    continue\n",
    "\n",
    "\n",
    "                self.classify(all_train_data, all_train_labels, all_test_data, all_test_labels, classifier_type)\n",
    "                #print \"Confusion matrix: \", self.conf_matrix\n",
    "                k_frequency.append(self.frequency)\n",
    "                k_accuracy.append(self.accuracy)\n",
    "                k_prf.append(self.prf)\n",
    "                k_conf_matrix.append(self.conf_matrix)\n",
    "\n",
    "                self.frequency = k_frequency\n",
    "                self.accuracy = k_accuracy\n",
    "                self.prf = k_prf\n",
    "                self.conf_matrix = k_conf_matrix \n",
    "        #print counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PCAClassifierPersonalisedHelper:\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_svm_accuracy(xy_data_list, iteration=3, classifier_type=1, users=[], min_days=30, encoded_layer_size=10):\n",
    "        # setting up the data\n",
    "        y_data = [d[4] for d in xy_data_list[0]]\n",
    "        x_data = []\n",
    "        for xy_data in xy_data_list:\n",
    "            x_data.append([[d[0], d[5]] for d in xy_data])\n",
    "\n",
    "        # classify\n",
    "        model = PCAClassifierPersonalised()\n",
    "        model.classify_personalised(x_data, y_data, iteration, classifier_type, users, min_days, encoded_layer_size)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureExtractionPCA:\n",
    "            \n",
    "    def __init__(self, encoded_layer_size):\n",
    "        self.pca = KernelPCA(n_components=encoded_layer_size, kernel='rbf')\n",
    "        \n",
    "    def encode_data(self, x_data):\n",
    "        f = self.pca.fit_transform(x_data)\n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# p.computePredictions(encoded_layer_size=1, classifier_type=1)\n",
    "# file_path = 'plots/pca/prediction_' + '_encoded_ls_' + str(1) + '_classifier_' + str(1) + '.pdf'\n",
    "# p.plotPredictions(include_mt=True, t_hist=t_hist, file_path=file_path)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial user count 6705\n",
      "User count 5616\n",
      "Min time ratio 0.500023134225\n",
      "Number of users 44\n",
      "Pos count: 900\n",
      "Neg count: 1489\n"
     ]
    }
   ],
   "source": [
    "all_models = [] # store models of all iterations (for debugging)\n",
    "time_window = 10\n",
    "t_hist = 14\n",
    "dropout_rate = 0.1\n",
    "p = Predictions(30)\n",
    "p.loadData(time_window=time_window, sd_ratio=0.25, t_hist=t_hist)\n",
    "p.prepareInput()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer size 1\n",
      "classifier type 1\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 2\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 3\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "layer size 2\n",
      "classifier type 1\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 2\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 3\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "layer size 3\n",
      "classifier type 1\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 2\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 3\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "layer size 4\n",
      "classifier type 1\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 2\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 3\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "layer size 5\n",
      "classifier type 1\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 2\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 3\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "layer size 6\n",
      "classifier type 1\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 2\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 3\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "layer size 7\n",
      "classifier type 1\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 2\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 3\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "layer size 8\n",
      "classifier type 1\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 2\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 3\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "layer size 9\n",
      "classifier type 1\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 2\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 3\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "layer size 10\n",
      "classifier type 1\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 2\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 3\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "layer size 11\n",
      "classifier type 1\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 2\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 3\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "layer size 12\n",
      "classifier type 1\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 2\n",
      "Number of users 24\n",
      "[0, 1]\n",
      "classifier type 3\n",
      "Number of users 24\n"
     ]
    }
   ],
   "source": [
    "for encoded_layer_size in range(1,21):\n",
    "    print 'layer size', encoded_layer_size\n",
    "    for classifier_type in [1,2,3]:\n",
    "        print 'classifier type', classifier_type\n",
    "        p.computePredictions(encoded_layer_size=encoded_layer_size, classifier_type=classifier_type)\n",
    "        file_path = 'plots/pca/prediction_' + '_encoded_ls_' + str(encoded_layer_size) + '_classifier_' + str(classifier_type) + '.pdf'\n",
    "        p.plotPredictions(include_mt=True, t_hist=t_hist, file_path=file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mn = ['IR 1','IR 2','IR 3','IR 1+2','IR 1+3','IR 2+3','IR 1+2+3']\n",
    "# ld = {1:'Sensitivity', 0:'Specificity'}\n",
    "# Plots.multiModel(models=p.models, model_names=mn, label_dict=ld, include_mt=True, t_hist=7, file_path='plots/prediction_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
